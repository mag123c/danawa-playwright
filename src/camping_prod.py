import os
from urllib.parse import urlencode
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup, Tag
import json
import re

def categorize_spec_items(parsed: list, spec_raw: str) -> dict:
    categories = {
        "í¬ê¸°": "í¬ê¸°",
        "í¬": "í¬ê¸°",
        "ì‚¬ì´ì¦ˆ": "í¬ê¸°",
        "ë¬´ê²Œ": "ë¬´ê²Œ",
        "ì¤‘ëŸ‰": "ë¬´ê²Œ",
        "ìš©ëŸ‰": "ìš©ëŸ‰",
        "ì†Œë¹„ì „ë ¥": "ì†Œë¹„ì „ë ¥",
        "ì†Œëª¨ì „ë ¥": "ì†Œë¹„ì „ë ¥",
        "ì „ë ¥": "ì†Œë¹„ì „ë ¥",
        "ì „ì••": "ì „ì••",
        "ì „ì›": "ì „ì›",
        "ì˜¨ë„": "ì˜¨ë„ë²”ìœ„",
        "ëƒ‰ì¥ì˜¨ë„": "ì˜¨ë„ë²”ìœ„",
        "ëƒ‰ë™ì˜¨ë„": "ì˜¨ë„ë²”ìœ„",
        "ë³´ëƒ‰íš¨ë ¥": "ë³´ëƒ‰íš¨ë ¥",
        "ë‚´ëƒ‰íš¨ë ¥": "ë³´ëƒ‰íš¨ë ¥",
        "ìƒ‰ìƒ": "ìƒ‰ìƒ",
        "ê¸°ëŠ¥": "ê¸°ëŠ¥",
        "íŠ¹ì§•": "íŠ¹ì§•",
        "ì¬ì§ˆ": "ì¬ì§ˆ",
        "ì†ŒìŒ": "ì†ŒìŒ"
    }

    result = {}
    seen = set()
    globally_seen_values = set()

    for key, value in parsed:
        values = [v.strip() for v in re.split(r'[,/]', value) if v.strip()]
        for val in values:
            if not val:
                continue

            # í¬ê¸° íŒ¨í„´ ì§ì ‘ íŒŒì•… (ë‚´ë¶€-, ì™¸ë¶€-, ê´„í˜¸ ì—†ëŠ” ì¼€ì´ìŠ¤ í¬í•¨)
            if re.search(r'(ë‚´ë¶€|ì™¸ë¶€)?[-\s]*\d{2,4}[\sxX\*]\d{2,4}[\sxX\*]\d{2,4}\s*(mm|cm)?', val, re.IGNORECASE):
                result.setdefault("í¬ê¸°", []).append(val)
                seen.add(("í¬ê¸°", val))
                globally_seen_values.add(val)
                continue

            # ì˜¨ë„ ë²”ìœ„: -20~80ë„, 15ë„ ì´í•˜ ë“±
            if re.search(r'[\-\d]{1,3}\s*[~\-]\s*\d{1,3}\s*[â„ƒÂ°CÂº]', val) or re.search(r'\d{1,3}\s*[â„ƒÂ°CÂº]\s*(ì´í•˜|ì´ìƒ)', val):
                result.setdefault("ë³´ëƒ‰íš¨ë ¥", []).append(val)
                seen.add(("ë³´ëƒ‰íš¨ë ¥", val))
                globally_seen_values.add(val)
                continue

            # ì†Œë¹„ì „ë ¥ ë‹¨ë… ì²˜ë¦¬ (W ë‹¨ìœ„ í¬í•¨, label ì—†ì´ë„ ì²˜ë¦¬)
            if re.search(r'\d{1,4}\s*(w|W)', val):
                result.setdefault("ì†Œë¹„ì „ë ¥", []).append(val)
                seen.add(("ì†Œë¹„ì „ë ¥", val))
                globally_seen_values.add(val)
                continue

            # ì†ŒìŒ ì²˜ë¦¬ (dB ë‹¨ìœ„ í¬í•¨, label ì—†ì´ë„ ì²˜ë¦¬)
            if re.search(r'\d{1,3}\s*dB', val, re.IGNORECASE):
                result.setdefault("ì†ŒìŒ", []).append(val)
                seen.add(("ì†ŒìŒ", val))
                globally_seen_values.add(val)
                continue

            # colon ê¸°ë°˜ ë¶„ë¦¬
            if ":" in val:
                sub_key, sub_val = map(str.strip, val.split(":", 1))
                for keyword, category in categories.items():
                    if keyword in sub_key and (category, sub_val) not in seen:
                        result.setdefault(category, []).append(sub_val)
                        seen.add((category, sub_val))
                        globally_seen_values.add(sub_val)
                        break
                else:
                    if val not in globally_seen_values:
                        result.setdefault("ê¸°íƒ€", []).append(val)
                        globally_seen_values.add(val)
            else:
                categorized = False
                for keyword, category in categories.items():
                    if keyword in key and (category, val) not in seen:
                        result.setdefault(category, []).append(val)
                        seen.add((category, val))
                        globally_seen_values.add(val)
                        categorized = True
                        break
                if not categorized and val not in globally_seen_values:
                    result.setdefault("ê¸°íƒ€", []).append(val)
                    globally_seen_values.add(val)

    return result

def remove_redundant_from_etc(spec_structured: dict) -> dict:
    labeled_values = set()
    for key, values in spec_structured.items():
        if key != "ê¸°íƒ€":
            for val in values:
                labeled_values.add(val.strip().lower())

    filtered_etc = []
    for val in spec_structured.get("ê¸°íƒ€", []):
        val_clean = val.strip().lower()
        if ":" in val_clean:
            try:
                _, sub_val = map(str.strip, val_clean.split(":", 1))
                if sub_val.lower() in labeled_values:
                    continue
            except:
                pass
        if val_clean not in labeled_values:
            filtered_etc.append(val)

    cleaned = {k: v for k, v in spec_structured.items() if k != "ê¸°íƒ€"}
    if filtered_etc:
        cleaned["ê¸°íƒ€"] = filtered_etc

    return cleaned



def extract_spec_html(spec_root: Tag) -> tuple[list[tuple[str, str]], str]:
    specs = []
    current_key = "ê¸°íƒ€"
    buffer = []

    text = spec_root.get_text(" / ", strip=True)

    # 1ì°¨: colon ìˆëŠ” í•­ëª© ë¨¼ì € ë¶„ë¦¬
    matches = re.findall(r'([^:/\n]+):\s*([^:/\n]+)', text)
    for k, v in matches:
        specs.append((k.strip(), v.strip()))

    # 2ì°¨: <span class="cm_mark"> ê¸°ë°˜ êµ¬ì¡° íŒŒì‹±
    for node in spec_root.children:
        if isinstance(node, str):
            buffer.append(node.strip())
        elif node.name == "span" and "cm_mark" in node.get("class", []):
            if buffer:
                joined = " / ".join(buffer).strip()
                if joined:
                    specs.append((current_key, joined))
                buffer = []
            current_key = node.get_text(strip=True).strip("[]")
        elif node.name in ["a", "b", "span"]:
            buffer.append(node.get_text(strip=True))

    if buffer:
        joined = " / ".join(buffer).strip()
        if joined:
            specs.append((current_key, joined))

    # 3ì°¨: "L" ë‹¨ìœ„ ë³´ì™„ ì¶”ì¶œ
    capacity_matches = re.findall(r'(\d{1,4}\.?\d*)\s*L', text, re.IGNORECASE)
    for cm in set(capacity_matches):
        specs.append(("ìš©ëŸ‰", f"{cm}L"))

    return specs, text


def scrape_products(group_code: str, cate_name: str, category_code: str, referer_cate_code: str,
                    depth: str = "2", end_page: int = 100, output_dir: str = "."):
    url = "https://prod.danawa.com/list/ajax/getProductList.ajax.php"
    all_items = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()

        for page_num in range(1, end_page + 1):
            print(f"ğŸ“„ í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì¤‘...")

            params = {
                "btnAllOptUse": "false",
                "page": str(page_num),
                "listCategoryCode": category_code,
                "categoryCode": category_code,
                "viewMethod": "LIST",
                "sortMethod": "BEST",
                "listCount": "90",
                "group": group_code,
                "depth": depth,
            }

            form_data = urlencode(params)

            response = page.request.post(
                url,
                data=form_data,
                headers={
                    "Content-Type": "application/x-www-form-urlencoded",
                    "Referer": f"https://prod.danawa.com/list/?cate={referer_cate_code}",
                    "X-Requested-With": "XMLHttpRequest"
                }
            )
            html = response.text()
            soup = BeautifulSoup(html, "html.parser")

            items = []
            for item in soup.select(".prod_item"):
                try:
                    if not item.get("id"):
                        continue

                    image_tag = item.select_one(".thumb_image img")
                    image_url = image_tag.get("src") or image_tag.get("data-original")
                    if image_url and not image_url.startswith("http"):
                        image_url = "https:" + image_url

                    spec_node = item.select_one(".spec-box .spec_list") or item.select_one(".spec_list")
                    spec_raw = spec_node.get_text(" / ", strip=True) if spec_node else ""
                    parsed_spec, _ = extract_spec_html(spec_node) if spec_node else ([], "")
                    categorized_spec = categorize_spec_items(parsed_spec, spec_raw)
                    categorized_spec = remove_redundant_from_etc(categorized_spec)
                    options = item.select(".spec_list a, .spec_list span, .spec_list div")
                    option_texts = [opt.get_text(strip=True) for opt in options]

                    items.append({
                        "id": item.get("id"),
                        "main_category": item.select_one("input[id^=productItem_categoryInfo]").get("value", ""),
                        "sub_category": cate_name,
                        "name": item.select_one(".prod_name a").get_text(strip=True),
                        "url": item.select_one(".prod_name a").get("href"),
                        "image": image_url,
                        "spec_raw": spec_raw,
                        "spec_structured": categorized_spec,
                        "price": item.select_one(".price_sect strong").get_text(strip=True),
                        "date": item.select_one(".meta_item.mt_date dd").get_text(strip=True) if item.select_one(".meta_item.mt_date dd") else None,
                        "option": option_texts,
                        "maker": item.select_one(".price_sect button[data-maker-name]").get("data-maker-name", "") if item.select_one(".price_sect button[data-maker-name]") else None,
                    })
                except Exception as e:
                    print(f"âš ï¸ íŒŒì‹± ì‹¤íŒ¨: {e}")

            if not items:
                print("ğŸš© ë” ì´ìƒ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break

            all_items.extend(items)

        browser.close()

    safe_name = "".join(c for c in cate_name if c.isalnum() or c in (' ', '_')).strip().replace(" ", "_")
    filename = f"products_{safe_name}.json"
    filepath = os.path.join(output_dir, filename)

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(all_items, f, indent=2, ensure_ascii=False)

    print(f"âœ… {cate_name} â†’ {len(all_items)}ê°œ ì €ì¥ ì™„ë£Œ: {filepath}")
    return all_items


if __name__ == "__main__":
    # group_code = "14"
    # cate_name = "ì°¨ëŸ‰ìš©ëƒ‰ì˜¨ëƒ‰ì¥ê³ "
    # cate_code = "36799"
    # ref_cate_code = "14236799"
    # end_page = 100
    # scrape_products(group_code, cate_name, cate_code, ref_cate_code, end_page=end_page)
    
    categories = [
        {
            "group_code": "13",
            "cate_name": "ì•„ì´ìŠ¤ë°•ìŠ¤",
            "cate_code": "42018",
            "ref_cate_code": "13342018",
            "depth": "3",
        },
        {
            "group_code": "13",
            "cate_name": "ì¿¨ëŸ¬ë°±",
            "cate_code": "42019",
            "ref_cate_code": "13342019",
            "depth": "3",
        },
        {
            "group_code": "14",
            "cate_name": "ì°¨ëŸ‰ìš©ëƒ‰ì˜¨ëƒ‰ì¥ê³ ",
            "cate_code": "36799",
            "ref_cate_code": "14236799",
            "depth": "3",
        },
    ]

    for category in categories:
        print(f"\nğŸ“¦ {category['cate_name']} ìˆ˜ì§‘ ì‹œì‘")
        scrape_products(
            group_code=category["group_code"],
            cate_name=category["cate_name"],
            category_code=category["cate_code"],
            referer_cate_code=category["ref_cate_code"],
            depth=category["depth"],
            end_page=100,
        )
    
    # group + depth + main_category = cate;
